# -*- coding: utf-8 -*-
"""convert2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u-Q_Anw0iYwcvHPslpnB0BOGlm6XQ6Tt

Resources: https://www.stxnext.com/blog/hugging-face-tutorial/
"""

import pandas as pd
import seaborn as sns
import pandas as pd
import seaborn as sns
import numpy as np
import sent2vec
from tqdm.auto import tqdm
from pandarallel import pandarallel
from transformers import AutoTokenizer, AutoModel
import torch

data_path = "DataPreprocessing/data"
corpus_data = "corpus_data"

# __setitem__ implements the assignment operation to self[key].
# if you call self[key] = value, Python will call self.__setitem__(key, value).

# This method stores duplicated values in lists under the same key automatically
# if it is given a key not in the dictionary then it adds it to the dict and adds an empty list as the value
# it it finds the list then it appends the value in the value list
# https://stackoverflow.com/questions/10664856/how-can-one-make-a-dictionary-with-duplicate-keys-in-python
class Dictlist(dict):
    def __setitem__(self, key, value):
        try:
            self[key]
        except KeyError:
            super(Dictlist, self).__setitem__(key, [])
        self[key].append(value)

"""Load the model<br>
odel = sent2vec.Sent2vecModel()<br>
odel.load_model(filename2)

ClinicalBERT

BioBert
"""

# extracted from hugging face

def load_model2():
    tokenizer = AutoTokenizer.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract")
    model = AutoModel.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract")
    return model, tokenizer

def get_embeddings_model(text, model, tokenizer, token_length=64):
    tokens=tokenizer(text,max_length=token_length,padding='max_length',return_tensors='pt', truncation=True)
    output=model(input_ids=tokens.input_ids,
             attention_mask=tokens.attention_mask).last_hidden_state
    return torch.mean(output,axis=1).detach().numpy()

"""Loading models"""

model, tokenizer = load_model2()

"""Getting embeddings"""

# Reading the text from the study_corpus and storing the embeddings
# in a new file.

df = pd.read_csv("corpus_data/study_corpus.txt", sep='\t', names=['id','text'])
data = Dictlist()
for i ,j in tqdm(df[["id", "text"]].values):
    data[i] = get_embeddings_model(text=j, model=model, tokenizer=tokenizer)


import numpy as np
np.save('corpus_embeddings_new/study_corpus_embeddings_data.npy', data)
print("saved.")