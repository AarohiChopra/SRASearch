# -*- coding: utf-8 -*-
"""Vectors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VqP2_mWnS0l9qBjSmdo_1L6mfmtqDobl
"""

import pandas as pd
import seaborn as sns
from gensim.models import FastText
from gensim.models import KeyedVectors
import sent2vec

filename = "/home/achopra/BioWordVec_PubMed_MIMICIII_d200.bin"
filename2 = "/home/achopra/BioSentVec_PubMed_MIMICIII-bigram_d700.bin"
data_path = "DataPreprocessing/data"
corpus_data = "corpus_data/"


experiment = pd.read_csv(f"{data_path}/experiment.csv", index_col=[0])

# Convert all datasets to txt files
#submission['submission_comment'].to_csv("submission_corpus.txt", index=False, header=False, sep='\t')

experiment['combined'] = experiment['title'] + "\n" + experiment['design_description'] + "\n" + experiment['library_name'] + "\n" + experiment['library_construction_protocol'] + "\n" + experiment['platform'] + "\n" + experiment['instrument_model'] + "\n" + experiment['platform_parameters']

experiment['combined'] = experiment['combined'].str.replace("<b>", "")
experiment['combined'] = experiment['combined'].str.replace("</b>", "")
experiment['combined'] = experiment['combined'].str.replace("<i>", "")
experiment['combined'] = experiment['combined'].str.replace("</i>", "")

experiment['combined'] = experiment['combined'].str.replace("\n", "")

from nltk.corpus import stopwords
import string
stop = stopwords.words('english')

def clean_df(df, col_name):
    # Preprocessing submission
    df[col_name] = df[col_name].apply(str)
    df[col_name] = df[col_name].str.lower()
    df[col_name] = df[col_name].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))
    df[col_name] = df[col_name].str.replace('[{}]'.format(string.punctuation), '')
    return df

experiment = clean_df(experiment, col_name='combined')

experiment = experiment.replace("None", "")
experiment = experiment[experiment.submission_accession != '-']

experiment[["submission_accession", 'combined']].to_csv("corpus_data/experiment_corpus.txt", index=False, header=False, sep='\t')

experiment

from nltk.corpus import stopwords
import string
stop = stopwords.words('english')

def clean_df(df, col_name):
    # Preprocessing submission
    df[col_name] = df[col_name].apply(str)
    df[col_name] = df[col_name].str.lower()
    df[col_name] = df[col_name].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stop))
    df[col_name] = df[col_name].str.replace('[{}]'.format(string.punctuation), '')
    return df

submission = clean_df(submission, col_name='submission_comment')
submission[["submission_accession", 'submission_comment']].to_csv("corpus_data/submission_corpus.txt", index=False, header=False, sep='\t')

# Study dataset
study['combined'] = study['study_title'] + "\n" + study['study_type'] + "\n" + study['study_abstract'] + "\n" + study['center_project_name']
study['combined'] = study['combined'].str.replace("<b>", "")
study['combined'] = study['combined'].str.replace("</b>", "")
study['combined'] = study['combined'].str.replace("<i>", "")
study['combined'] = study['combined'].str.replace("</i>", "")

study = clean_df(study, col_name='combined')

study[['submission_accession','combined']].to_csv("corpus_data/study_corpus.txt", index=False, header=False, sep='\t')

sra = sra[sra.submission_accession != '-']

sra = sra.replace("None", "")

sra['combined'] = sra['experiment_title'] + "\n" + sra['library_strategy'] + "\n" + sra['description']

sra.head()

sra = sra[sra.combined != "\n\n"]
sra = sra[sra.combined != "\n"]

sra.combined = sra.combined.str.replace("\\n\\n", "")

sra.head()

sra_ft = clean_df(sra, col_name='combined')

sra

sra[["submission_accession", "combined"]].to_csv("corpus_data/sra_corpus.txt", index=False, header=False, sep='\t')

sra_ft.head()

sra_ft = sra_ft.replace("None", "")

sra_ft = sra_ft[sra_ft.submission_accession != '-']

sra_ft['study_abstract'] = sra_ft.study_abstract.str.replace("<b>", "")
sra_ft['study_abstract']  = sra_ft.study_abstract.str.replace("<i>", "")
sra_ft['study_abstract']  = sra_ft.study_abstract.str.replace("</b>", "")
sra_ft['study_abstract']  = sra_ft.study_abstract.str.replace("</i>", "")

sra_ft['combined'] = sra_ft['design_description'] + "\n" + sra_ft['study_abstract']

sra_ft['combined'] = sra_ft.combined.str.replace("\n", "")

sra_ft = clean_df(sra_ft, col_name='combined')

sra_ft[["submission_accession", "combined"]].to_csv("corpus_data/sra_ft_corpus.txt", index=False, header=False, sep='\t')

#experiment['combined'] = experiment['title'] + "\n" +





# Load the model
model = sent2vec.Sent2vecModel()
model.load_model(filename2)

submission.head()

from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim

model = SentenceTransformer('paraphrase-MiniLM-L6-v2')


#Our sentences we like to encode
sentences = ['This framework generates embeddings for each input sentence',
    'where']

embeddings_data = {}

#Sentences are encoded by calling model.encode()
embeddings = model.encode(sentences)

#Print the embeddings
for sentence, embedding in zip(sentences, embeddings):
    embeddings_data[sentence] = embedding

a = embeddings_data['where']
b =  embeddings_data['This framework generates embeddings for each input sentence']

cos_sim(a, b)

sample = pd.read_csv(f"{data_path}/sample.csv", index_col=[0])

sample.head(1000).values

sample['sample_attribute'] = sample['sample_attribute'].str.replace("||", "")
sample['sample_attribute'] = sample['sample_attribute'].str.replace(":", "")
sample['sample_attribute'] = sample['sample_attribute'].str.replace(";", "")