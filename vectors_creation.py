# -*- coding: utf-8 -*-
"""Vectors-Creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OS4_BIst6DsWIGkIB69-IAFTw_u78j1y
"""

import pandas as pd
import seaborn as sns
from gensim.models import FastText
from gensim.models import KeyedVectors
import sent2vec

filename = "/home/achopra/BioWordVec_PubMed_MIMICIII_d200.bin"
filename2 = "/home/achopra/BioSentVec_PubMed_MIMICIII-bigram_d700.bin"
data_path = "DataPreprocessing/data"
corpus_data = "corpus_data"

class Dictlist(dict):
    def __setitem__(self, key, value):
        try:
            self[key]
        except KeyError:
            super(Dictlist, self).__setitem__(key, [])
        self[key].append(value)

# Load the model
model = sent2vec.Sent2vecModel()
model.load_model(filename2)


def load_model2():
    tokenizer = AutoTokenizer.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract")
    model = AutoModel.from_pretrained("microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract")
    return model, tokenizer


def get_embeddings_model(text, model, tokenizer, token_length=64):
    tokens=tokenizer(text,max_length=token_length,padding='max_length',return_tensors='pt', truncation=True)
    output=model(input_ids=tokens.input_ids,
             attention_mask=tokens.attention_mask).last_hidden_state
    return torch.mean(output,axis=1).detach().numpy()

df = pd.read_csv("corpus_data/sra_corpus.txt", sep='\t', names=['id','text'])

data = Dictlist()
for i ,j in df[["id", "text"]].values:
    data[i] = model.embed_sentences([j])

import numpy as np
np.save('corpus_embeddings/sra_embeddings_data.npy', data)

# pickling:  the process of converting a Python object into a byte stream to store it in a file/database
# allow_pickle='TRUE', allowing us to see some raw data
read_dictionary = np.load('corpus_embeddings/sra_embeddings_data.npy',allow_pickle='TRUE').item()



# This piece of code takes in sentences, creates the embeddings
# and finally add it to a dictionary with the key as a sentence and
# value as the embedding

from sentence_transformers import SentenceTransformer
from sentence_transformers.util import cos_sim

model = SentenceTransformer('paraphrase-MiniLM-L6-v2')


# Our sentences we like to encode
sentences = ['This framework generates embeddings for each input sentence',
    'where']

embeddings_data = {}

# Sentences are encoded by calling model.encode()
embeddings = model.encode(sentences)

# Print the embeddings
for sentence, embedding in zip(sentences, embeddings):
    embeddings_data[sentence] = embedding

a = embeddings_data['where']
b =  embeddings_data['This framework generates embeddings for each input sentence']

cos_sim(a, b)